1. mergesort
    import java.util.*;

    class MergeSort {
        static void mergeSort(int a[], int l, int h) {
            if (l < h) {
                int m = (l + h) / 2;
                mergeSort(a, l, m);
                mergeSort(a, m + 1, h);
                merge(a, l, m, h);
            }
        }

        static void merge(int a[], int l, int m, int h) {
            int i = l, j = m + 1, k = l;
            int c[] = new int[a.length];
            while (i <= m && j <= h) c[k++] = (a[i] <= a[j]) ? a[i++] : a[j++];
            while (i <= m) c[k++] = a[i++];
            while (j <= h) c[k++] = a[j++];
            for (i = l; i <= h; i++) a[i] = c[i];
        }

        public static void main(String[] args) {
            Scanner sc = new Scanner(System.in);
            int n = sc.nextInt();
            int a[] = new int[n];
            for (int i = 0; i < n; i++) a[i] = sc.nextInt();
            mergeSort(a, 0, n - 1);
            for (int x : a) System.out.print(x + " ");
        }
    }

import matplotlib.pyplot as plt
import numpy as np

# Input sizes
n_values = np.linspace(1, 1000, 100)

# Time complexities
best_case = n_values * np.log2(n_values)
average_case = n_values * np.log2(n_values)
worst_case = n_values ** np.log2(n_values)

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(n_values, best_case, label='Best Case (O(n log n))', color='green')
plt.plot(n_values, average_case, label='Average Case (O(n log n))', linestyle='--', color='blue')
plt.plot(n_values, worst_case, label='Worst Case (O(n log n))', linestyle='-.', color='red')

plt.title('Merge Sort Time Complexities')
plt.xlabel('Input Size (n)')
plt.ylabel('Operations')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

2. quicksort 
import java.util.*;

class QuickSort {
    static void quickSort(int a[], int l, int h) {
        if (l < h) {
            int p = partition(a, l, h);
            quickSort(a, l, p - 1);
            quickSort(a, p + 1, h);
        }
    }

    static int partition(int a[], int l, int h) {
        int pivot = a[l], i = l, j = h;
        while (i < j) {
            while (i <= h && a[i] <= pivot) i++;
            while (a[j] > pivot) j--;
            if (i < j) { int t = a[i]; a[i] = a[j]; a[j] = t; }
        }
        int t = a[l]; a[l] = a[j]; a[j] = t;
        return j;
    }

    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);
        int n = sc.nextInt(), a[] = new int[n];
        for (int i = 0; i < n; i++) a[i] = sc.nextInt();
        quickSort(a, 0, n - 1);
        for (int x : a) System.out.print(x + " ");
    }
}


import matplotlib.pyplot as plt
import numpy as np

# Input sizes
n_values = np.linspace(1, 1000, 100)

# Time complexities
best_case = n_values * np.log2(n_values)
average_case = n_values * np.log2(n_values)
worst_case = n_values ** 2

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(n_values, best_case, label='Best Case (O(n log n))', color='green')
plt.plot(n_values, average_case, label='Average Case (O(n log n))', linestyle='--', color='blue')
plt.plot(n_values, worst_case, label='Worst Case (O(n²))', linestyle='-.', color='red')

plt.title('quick Sort Time Complexities')
plt.xlabel('Input Size (n)')
plt.ylabel('Operations')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


3. articulation point
def dfs (node, adj, visited):
    visited[node] = True
    for neighbor in adj[node]:
        if not visited[neighbor]:
            dfs(neighbor, adj, visited)

def ap (V, edges):
    adj =[[] for _ in range(V)]
    for u, v  in edges:
        adj[u].append(v)
        adj[v].append(u)
    res = []
    for i in range(V):
        visited = [False] * V
        visited[i] = True
        comp = 0
        for it in range(V):
            if not visited[it]:
                dfs(it, adj, visited)
                comp += 1
            if comp > 1:
                break
        if comp > 1:
            res.append(i)
    return res if res else [-1]

V = int(input('Enter the no of vertices:'))
E = int(input('Enter the no of edges:'))

edges = [list(map(int, input('Enter the pair(u v)').split())) for _ in range(E)]
print('AP : ', *ap(V, edges))

4. jobs

import java.util.*;

class JobSquencing {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);

        int n = sc.nextInt();
        int[] d = new int[n], p = new int[n];

        for (int i = 0; i < n; i++) d[i] = sc.nextInt();
        for (int i = 0; i < n; i++) p[i] = sc.nextInt();

        Integer[] jobs = new Integer[n];
        for (int i = 0; i < n; i++) jobs[i] = i;

        Arrays.sort(jobs, (a, b) -> p[b] - p[a]);

        int maxDeadline = Arrays.stream(d).max().getAsInt();
        int[] slot = new int[maxDeadline + 1];
        Arrays.fill(slot, -1);

        int totalProfit = 0, count = 0;

        for (int job : jobs) {
            for (int t = d[job]; t > 0; t--) {
                if (slot[t] == -1) {
                    slot[t] = job;
                    totalProfit += p[job];
                    count++;
                    break;
                }
            }
        }

        System.out.println("Jobs selected and their profits:");
        for (int t = 1; t <= maxDeadline; t++)
            if (slot[t] != -1)
                System.out.println("Job" + (slot[t] + 1) + " -> Profit: " + p[slot[t]]);

        System.out.println("\nTotal Jobs Done: " + count);
        System.out.println("Total Profit: " + totalProfit);
        sc.close();
    }
}



5. knapsack

import java.util.*;

class GreedyKnapsack{
    static class Item{
        double p, w, r;
        Item(double p, double w){this.p=p; this.w=w; this.r = p/w;}
    }

    public static void main(String[] args){
        Scanner sc = new Scanner(System.in);
        System.out.println("Enter noof items and cap: ");
        int n=sc.nextInt(); double cap = sc.nextDouble();
        
        Item[] items = new Item[n];
        for (int i=0; i<n; i++)
            items[i] = new Item(sc.nextDouble(), sc.nextDouble());
        Arrays.sort(items, (a,b) -> Double.compare(b.r, a.r));

        double profit=0, rem=cap;
        for (Item it : items){
            if (rem == 0 ) break;
            double take = Math.min(1, rem/it.w);
            profit += take * it.p;
            rem -= take * it.w;
            System.out.printf("Take %.2f of item (p=%.1f, w=%.1f)\n", take, it.p, it.w);
        }
        System.out.println(profit);
                sc.close();

    }
}


6.. dijkstras

INF = 999

def dijkstra(g, n, s):
    d, vis = [INF]*n, [0]*n
    d[s] = 0
    for _ in range(n):
        u = min((i for i in range(n) if not vis[i]), key=lambda i: d[i], default=-1)
        if u == -1 or d[u] == INF: break
        vis[u] = 1
        for v in range(n):
            if not vis[v] and g[u][v] != INF:
                d[v] = min(d[v], d[u] + g[u][v])
    return d

n = int(input())
g = [list(map(int, input().split())) for _ in range(n)]
s = int(input())
print(*dijkstra(g, n, s))
 


7.. obst

def optimal_bst(keys, freq):
    n = len(keys)
    if not n: return 0
    dp = [[0]*n for _ in range(n)]

    for i in range(n):
        dp[i][i] = freq[i]

    for l in range(2, n+1):
        for i in range(n-l+1):
            j = i + l - 1
            dp[i][j] = float('inf')
            freq_sum = sum(freq[i:j+1])
            for r in range(i, j+1):
                left = dp[i][r-1] if r > i else 0
                right = dp[r+1][j] if r < j else 0
                dp[i][j] = min(dp[i][j], left + right + freq_sum)
    return dp[0][n-1]

if __name__ == "__main__":
    keys = list(map(int, input("Enter keys: ").split()))
    freq = list(map(int, input("Enter frequencies: ").split()))
    print("Optimal cost of OBST:", optimal_bst(keys, freq))


8.. prims 
INF = 9999

def prims(cost, n):
    near = [None] * n
    edges = []
    total = 0
    u, v = min([(i, j) for i in range(n) for j in range(n) if i != j], key=lambda x: cost[x[0]][x[1]])
    edges.append((u, v))
    total += cost[u][v]
    near[u] = near[v] = -1
    for i in range(n):
        if near[i] != -1:
            near[i] = u if cost[i][u] < cost[i][v] else v
    for _ in range(n - 2):
        k = min([j for j in range(n) if near[j] != -1], key=lambda j: cost[j][near[j]])
        edges.append((k, near[k]))
        total += cost[k][near[k]]
        near[k] = -1
        for i in range(n):
            if near[i] != -1 and cost[i][k] < cost[i][near[i]]:
                near[i] = k
    return edges, total

n = int(input("Enter number of vertices: "))
print("Enter cost matrix:")
cost = [list(map(int, input().split())) for _ in range(n)]
edges, total = prims(cost, n)
print("Min Spanning Tree edges:")
print(*edges)
print("Total cost of MST:", total)

java

import java.util.*;

public class prims5 {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);
        int ne = 100;
        System.out.print("Enter number of vertices: ");
        int n = sc.nextInt();

        int[][] graph = new int[n][n];
        System.out.println("Enter adjacency matrix (use " + ne + " for no edge):");
        for (int i = 0; i < n; i++) {
            for (int j = 0; j < n; j++) {
                graph[i][j] = sc.nextInt();
                if (i == j) graph[i][j] = 0;
            }
        }

        boolean[] inMST = new boolean[n];
        int[] key = new int[n];
        Arrays.fill(key, ne);
        key[0] = 0;

        int[] parent = new int[n];
        parent[0] = -1;

        for (int count = 0; count < n - 1; count++) {
            int u = -1, min = ne;
            for (int v = 0; v < n; v++) {
                if (!inMST[v] && key[v] < min) {
                    min = key[v];
                    u = v;
                }
            }

            inMST[u] = true;
            for (int v = 0; v < n; v++) {
                if (!inMST[v] && graph[u][v] != ne && graph[u][v] < key[v]) {
                    key[v] = graph[u][v];
                    parent[v] = u;
                }
            }
        }

        System.out.println("Edge \tWeight");
        for (int i = 1; i < n; i++) {
            System.out.println(parent[i] + " - " + i + "\t" + graph[i][parent[i]]);
        }
    }
}


9.. Kruskal 
def kruskal(V, edges):
    p = list(range(V)); mst = []; cost = 0
    def f(i): return i if p[i]==i else f(p[i])
    edges.sort()
    for w,u,v in edges:
        if f(u)!=f(v):
            mst.append((u,v)); cost+=w
            p[f(v)] = f(u)
            if len(mst)==V-1: break
    return cost, mst

V = int(input())
E = int(input())
edges = [tuple(map(int, input().split()))[::-1] for _ in range(E)]
c,m = kruskal(V, edges)
print("Cost:", c)
print("Edges:", m)

----------------------ds------------
pca
-----------
# Demonstration of PCA (Dimensionality Reduction)
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Step 1: Load dataset
data = load_iris()
X = data.data
y = data.target

# Step 2: Standardize features
X_scaled = StandardScaler().fit_transform(X)

# Step 3: Apply PCA (reduce from 4D to 2D)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Step 4: Plot the data before and after PCA
plt.figure(figsize=(10, 4))

# Before PCA (first 2 features)
plt.subplot(1, 2, 1)
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='rainbow')
plt.title("Before PCA")

# After PCA (reduced 2D data)
plt.subplot(1, 2, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='rainbow')
plt.title("After PCA")

plt.show()


rfe
--------------------
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
import matplotlib.pyplot as plt

# Load dataset
data = load_iris()
X, y = data.data, data.target
feature_names = data.feature_names

# Create Logistic Regression model
model = LogisticRegression(max_iter=200)

# Apply RFE to select 2 best features
rfe = RFE(model, n_features_to_select=2)
X_rfe = rfe.fit_transform(X, y)

# Print selected features
print("Selected Features:")
for name, selected in zip(feature_names, rfe.support_):
    if selected:
        print(name)

# Plot before and after RFE
plt.figure(figsize=(12, 5))

# Before RFE
plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow')
plt.title("Before RFE (All Features)")
plt.xlabel(feature_names[0])
plt.ylabel(feature_names[1])

# After RFE
plt.subplot(1, 2, 2)
plt.scatter(X_rfe[:, 0], X_rfe[:, 1], c=y, cmap='rainbow')
plt.title("After RFE (Selected Features)")
plt.xlabel("petal length")
plt.ylabel("petal width")

plt.show()

mlr
------------------
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import *

# Dataset
data = {
    "Age": [25, 30, 35, 40, 50, 60],
    "BMI": [22, 25, 27, 28, 30, 32],
    "Exercise_Hours": [6, 5, 4, 3, 2, 1],
    "Health_Score": [85, 82, 78, 75, 70, 65]
}

df = pd.DataFrame(data)

# Independent variables and dependent variable
X = df[["Age", "BMI", "Exercise_Hours"]]
y = df["Health_Score"]

# Fit Multiple Linear Regression
model = LinearRegression()
model.fit(X, y)

# Coefficients and Intercept
print("Intercept (β0):", model.intercept_)
print("Coefficients (β1, β2, β3):", model.coef_)

# Predictions
y_pred = model.predict(X)

# Model Evaluation
r2 = r2_score(y, y_pred)
rmse = mean_squared_error(y, y_pred)

print("\nPredictions:", y_pred)
print("R² Score:", r2)
print("RMSE:", rmse)

---------
slr
-----------------
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import *

# Dataset
data = {
    "Age": [25, 30, 35, 40, 50, 60],
    "Health_Score": [85, 82, 78, 75, 70, 65]
}

df = pd.DataFrame(data)

# Select only ONE independent variable (Age)
X = df[["Age"]]         # 2D array → independent variable
y = df["Health_Score"]  # dependent variable

# Create and train the model
model = LinearRegression()
model.fit(X, y)

# Coefficient and Intercept
print("Intercept (β0):", model.intercept_)
print("Coefficient (β1):", model.coef_)

# Make predictions
y_pred = model.predict(X)

# Model evaluation
r2 = r2_score(y, y_pred)
mse = mean_squared_error(y, y_pred)

print("\nPredictions:", y_pred)
print("R² Score:", r2)
print("MSE:", mse)

knn
---------------------
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import *

iris = load_iris()
X = iris.data   
y = iris.target  

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train) 

y_pred = knn.predict(X_test_scaled)

print("Accuracy of KNN model:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))


randomforest
-------------------
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import *
import matplotlib.pyplot as plt

# ---- Small Bank Dataset ----
data = {
    "Age": [25, 45, 35, 50],
    "Income": [30000, 80000, 50000, 90000],
    "LoanAmount": [10000, 20000, 14000, 25000],
    "Loan_Status": [0, 1, 0, 1]
}

df = pd.DataFrame(data)
X = df.drop("Loan_Status", axis=1)
y = df["Loan_Status"]

# Split ensuring both classes are in each set
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.5, random_state=42, stratify=y
)

# Train model
rf = RandomForestClassifier(n_estimators=5, random_state=42)
rf.fit(X_train, y_train)

# Predict
y_pred = rf.predict(X_test)

# Evaluate
print("✅ Accuracy:", accuracy_score(y_test, y_pred))
print("\n Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\n Classification Report:\n", classification_report(y_test, y_pred))


decisiontree
-----------------
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
clf = DecisionTreeClassifier(random_state=23)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Decision Tree Classifier: {accuracy}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred,  target_names=iris.target_names))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred)

-------------- 
kmeans
------------
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans

# Load the iris dataset
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)

# Apply K-Means
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
df['Cluster'] = kmeans.fit_predict(df)

# Add actual target to compare
df['Actual'] = iris.target

# Visualize using Seaborn
sns.scatterplot(x=df['sepal length (cm)'], y=df['sepal width (cm)'],
                hue=df['Cluster'], palette='viridis', s=80)
plt.title("K-Means Clustering on Iris Dataset")
plt.show()

# Compare with actual classes
sns.scatterplot(x=df['sepal length (cm)'], y=df['sepal width (cm)'],
                hue=df['Actual'], palette='coolwarm', s=80)
plt.title("Actual Iris Classes")
plt.show()
